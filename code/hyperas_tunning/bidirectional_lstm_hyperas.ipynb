{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import warnings\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import warnings\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'acti1v': hp. choice('acti1v', ['leaky' , 'prelu', 'tanh']),\n",
      "        'alphav1': hp.uniform('alphav1', 0, 1),\n",
      "        'acti2v': hp. choice('acti2v', ['leaky','prelu','tanh']),\n",
      "        'alphav1_1': hp.uniform('alphav1_1', 0, 1),\n",
      "        'kernel_initializer2': hp.choice('kernel_initializer2',  [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] ),\n",
      "        'kernel_initializer2_1': hp.choice('kernel_initializer2_1',  [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] ),\n",
      "        'kernel_initializer2_2': hp.choice('kernel_initializer2_2',  [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] ),\n",
      "        'kernel_initializer2_3': hp.choice('kernel_initializer2_3',  [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] ),\n",
      "        'alphav1_2': hp.uniform('alphav1_2', 0, 1),\n",
      "        'alphav1_3': hp.uniform('alphav1_3', 0, 1),\n",
      "        'neurons2': hp.choice('neurons2', [x for x in range(256, 1024, 32)]),\n",
      "        'neurons2_1': hp.choice('neurons2_1', [x for x in range(256, 1024, 32)]),\n",
      "        'lr': hp.choice('lr', [0.005,0.001,0.0005, 0.0001]),\n",
      "        'batch_size': hp.choice('batch_size', [x for x in range(16, 128, 16)]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \n",
      "   3: import tensorflow as tf\n",
      "   4: from tensorflow import keras\n",
      "   5: from sklearn.preprocessing import StandardScaler\n",
      "   6: from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
      "   7: import pandas as pd\n",
      "   8: import pickle\n",
      "   9: import numpy as np\n",
      "  10: import matplotlib.pyplot as plt\n",
      "  11: from sklearn.model_selection import train_test_split\n",
      "  12: from keras import backend as K\n",
      "  13: \n",
      "  14: import warnings\n",
      "  15: warnings.filterwarnings('ignore')\n",
      "  16: \n",
      "  17: path_origin, path_processed, models_path, cells = \"../data/original_data/\", \"../data/processed_data/\", \"../data/processed_data/models/\", [f'\\Cell{x}\\*' for x in range(1, 9)]\n",
      "  18: \n",
      "  19: df_desc_final_pickle_load = pd.read_pickle(path_processed + 'df_desc_final.pkl')\n",
      "  20: df_desc_final_pickle = df_desc_final_pickle_load[df_desc_final_pickle_load.RUL >= 0 ]\n",
      "  21: df_desc_final_pickle[\"RUL\"] = df_desc_final_pickle[\"RUL\"].astype(float)\n",
      "  22: \n",
      "  23: x, y, num_steps, percentage_and_rul = [], [], 10, []\n",
      "  24: \n",
      "  25: for i in range(1, 9):\n",
      "  26:     batery_df = df_desc_final_pickle[df_desc_final_pickle.index.get_level_values(0) == i]\n",
      "  27:     batery_df.index = pd.MultiIndex.from_tuples([(1, x) for x in range(1, batery_df.shape[0] + 1)])\n",
      "  28:     for j in range(1, len(batery_df) - num_steps):\n",
      "  29:         x.append(np.array(batery_df.loc[(1, slice(j,num_steps+j)), :].drop(columns=['RUL']).values))\n",
      "  30:         y.append(np.array([batery_df.loc[(1, slice(j,num_steps+j)), :].RUL.values[-1], i]))\n",
      "  31:         \n",
      "  32: x, y = np.array(x), np.array(y)\n",
      "  33: X_train,X_test,y_train_mod,y_test_mod = train_test_split(x, y,test_size=0.2, random_state=42, stratify=y[:,1])\n",
      "  34: y_train, y_test = y_train_mod[:,0].reshape(-1,1), y_test_mod[:,0].reshape(-1,1)\n",
      "  35: X_train, y_train = np.stack(X_train, axis = 0), np.stack(y_train, axis = 0)\n",
      "  36: X_test, y_test = np.stack(X_test, axis = 0), np.stack(y_test, axis = 0)\n",
      "  37: \n",
      "  38: scaler = StandardScaler()\n",
      "  39: X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
      "  40: X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
      "  41: \n",
      "  42: lista_resultados = []\n",
      "  43: \n",
      "  44: \n",
      "  45: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     warnings.filterwarnings('ignore')\n",
      "   5: \n",
      "   6:     def customLoss(true,pred):\n",
      "   7:         diff = pred - true\n",
      "   8:         greater = K.greater(diff,0)\n",
      "   9:         greater = K.cast(greater, K.floatx()) \n",
      "  10:         greater = greater + 1                \n",
      "  11:         return K.mean(K.square(diff))*greater\n",
      "  12:     \n",
      "  13:     nombre = models_path + 'tryals_bidirectional_lstm_' + str(len(lista_resultados)) + '.h5'\n",
      "  14:     checkpoint_rb = keras.callbacks.ModelCheckpoint(nombre, verbose=0, save_best_only=True)\n",
      "  15: \n",
      "  16:     alphav1, alphav2 = 0, 0\n",
      "  17:     acti1v = space['acti1v']\n",
      "  18:     if acti1v == 'leaky':  \n",
      "  19:         alphav1 = space['alphav1']\n",
      "  20:         acti1 = keras.layers.LeakyReLU(alpha=alphav1)\n",
      "  21:     if acti1v == 'prelu': acti1 = tf.keras.layers.PReLU('zeros')\n",
      "  22:     if acti1v == 'relu': acti1 = 'relu'\n",
      "  23:     if acti1v == 'tanh': acti1 = 'tanh'\n",
      "  24: \n",
      "  25:     acti2v = space['acti2v']\n",
      "  26:     if acti2v == 'leaky':  \n",
      "  27:         alphav2 = space['alphav1_1']\n",
      "  28:         acti2 = keras.layers.LeakyReLU(alpha=alphav2)\n",
      "  29:     if acti2v == 'prelu': acti2 = tf.keras.layers.PReLU('zeros')\n",
      "  30:     if acti2v == 'relu': acti2 = 'relu'\n",
      "  31:     if acti2v == 'tanh': acti2 = 'tanh'\n",
      "  32: \n",
      "  33:     kernel_initializer1, kernel_initializer2 = space['kernel_initializer2'], space['kernel_initializer2_1']\n",
      "  34:     recurrent_initializer1, recurrent_initializer2 = space['kernel_initializer2_2'], space['kernel_initializer2_3']\n",
      "  35:     recurrent_dropout1, recurrent_dropout2 = space['alphav1_2'], space['alphav1_3']\n",
      "  36:     neurons1, neurons2 = space['neurons2'], space['neurons2_1']\n",
      "  37:     np.random.seed(26), tf.random.set_seed(26);\n",
      "  38: \n",
      "  39:     model = keras.Sequential()\n",
      "  40:     forward_layer = keras.layers.LSTM(neurons1, activation = acti1, kernel_initializer=kernel_initializer1, recurrent_initializer=recurrent_initializer1, \n",
      "  41:                                         recurrent_dropout = recurrent_dropout1)\n",
      "  42:     backward_layer = keras.layers.LSTM(neurons2, activation = acti2, kernel_initializer=kernel_initializer2, recurrent_initializer=recurrent_initializer2, \n",
      "  43:                                         recurrent_dropout = recurrent_dropout2, go_backwards = True)\n",
      "  44: \n",
      "  45:     model.add(keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer, input_shape=(X_train.shape[1], X_train.shape[2])  ))\n",
      "  46:     model.add(keras.layers.Dense(1, activation = 'relu'))\n",
      "  47: \n",
      "  48:     lr = space['lr']\n",
      "  49:     model.compile(loss = customLoss,  optimizer = keras.optimizers.Adam(learning_rate = lr), metrics = ['mse'])\n",
      "  50:     \n",
      "  51:     print('=======================================================================================================================================================================================')\n",
      "  52:     batch_size = space['batch_size']\n",
      "  53:     history = model.fit(X_train, y_train,\n",
      "  54:               batch_size = batch_size,\n",
      "  55:               epochs=500, validation_split=0.2, verbose=0, callbacks = [checkpoint_rb])\n",
      "  56: \n",
      "  57:     val_acc_per_epoch = history.history['val_loss']\n",
      "  58:     best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))\n",
      "  59:     lista_resultados.append(history.history['val_loss'][best_epoch])\n",
      "  60:     print('El trial con menor error es el:', np.argmin(lista_resultados))\n",
      "  61:     params_dic = {'neurons1': neurons1, 'neurons2': neurons2, 'acti1v': acti1v, 'acti2v': acti2v, 'alphav1': alphav1, 'alphav2': alphav2, 'kernel_initializer1': kernel_initializer1,\n",
      "  62:                     'kernel_initializer2': kernel_initializer2, 'recurrent_initializer1': recurrent_initializer1, 'recurrent_initializer2': recurrent_initializer2,\n",
      "  63:                     'recurrent_dropout1': recurrent_dropout1, 'recurrent_dropout2': recurrent_dropout2, 'lr': lr, 'batch_size': batch_size}\n",
      "  64:     print(params_dic)\n",
      "  65:     \n",
      "  66:     return {'loss' : history.history['val_loss'][best_epoch], 'status' : STATUS_OK, 'model' : model, 'metrics' : np.sqrt(history.history['val_mse'][best_epoch])}\n",
      "  67: \n",
      "=======================================================================================================================================================================================\n",
      "  0%|          | 0/75 [00:02<?, ?trial/s, best loss=?]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FA4619D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FA4619D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FA4619D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FA4619D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                       \n",
      "0                                                     \n",
      "{'neurons1': 288, 'neurons2': 960, 'acti1v': 'tanh', 'acti2v': 'leaky', 'alphav1': 0, 'alphav2': 0.06536980304050743, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.01781861513722749, 'recurrent_dropout2': 0.8450298764739911, 'lr': 0.005, 'batch_size': 32}\n",
      "=======================================================================================================================================================================================\n",
      "  1%|▏         | 1/75 [16:50<20:46:02, 1010.30s/trial, best loss: 3926020.5]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FBFE6F70> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FBFE6F70>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FBFE6F70> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180FBFE6F70>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                             \n",
      "1                                                                           \n",
      "{'neurons1': 448, 'neurons2': 384, 'acti1v': 'prelu', 'acti2v': 'prelu', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.9662681038993752, 'recurrent_dropout2': 0.011106434718081704, 'lr': 0.001, 'batch_size': 112}\n",
      "=======================================================================================================================================================================================\n",
      "  3%|▎         | 2/75 [20:21<10:56:45, 539.80s/trial, best loss: 32894.62890625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001808BDE64C0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001808BDE64C0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001808BDE64C0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001808BDE64C0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                 \n",
      "1                                                                               \n",
      "{'neurons1': 320, 'neurons2': 928, 'acti1v': 'prelu', 'acti2v': 'prelu', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.7114247520786349, 'recurrent_dropout2': 0.5623815057565213, 'lr': 0.0005, 'batch_size': 112}\n",
      "=======================================================================================================================================================================================\n",
      "  4%|▍         | 3/75 [27:52<9:59:19, 499.43s/trial, best loss: 32894.62890625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001808EAC68B0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001808EAC68B0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001808EAC68B0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001808EAC68B0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                \n",
      "1                                                                              \n",
      "{'neurons1': 416, 'neurons2': 736, 'acti1v': 'prelu', 'acti2v': 'prelu', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.9975531408239052, 'recurrent_dropout2': 0.9912013870496312, 'lr': 0.001, 'batch_size': 80}\n",
      "=======================================================================================================================================================================================\n",
      "  5%|▌         | 4/75 [35:21<9:27:23, 479.49s/trial, best loss: 32894.62890625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018096A6CEE0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018096A6CEE0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018096A6CEE0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018096A6CEE0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                \n",
      "1                                                                              \n",
      "{'neurons1': 960, 'neurons2': 640, 'acti1v': 'tanh', 'acti2v': 'tanh', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.8364280725819752, 'recurrent_dropout2': 0.9532739136720357, 'lr': 0.0001, 'batch_size': 48}\n",
      "=======================================================================================================================================================================================\n",
      "  7%|▋         | 5/75 [53:04<13:24:45, 689.79s/trial, best loss: 32894.62890625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180A6DDE3A0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180A6DDE3A0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180A6DDE3A0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180A6DDE3A0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                 \n",
      "1                                                                                 \n",
      "{'neurons1': 576, 'neurons2': 736, 'acti1v': 'leaky', 'acti2v': 'tanh', 'alphav1': 0.9284679779637944, 'alphav2': 0, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.6264617360166798, 'recurrent_dropout2': 0.1904090881738011, 'lr': 0.0005, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      "  8%|▊         | 6/75 [1:19:35<19:05:56, 996.47s/trial, best loss: 32894.62890625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AB5FB700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AB5FB700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AB5FB700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AB5FB700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "1                                                                                 \n",
      "{'neurons1': 768, 'neurons2': 352, 'acti1v': 'tanh', 'acti2v': 'prelu', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.9553237496658106, 'recurrent_dropout2': 0.7190372049353905, 'lr': 0.0001, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      "  9%|▉         | 7/75 [1:42:28<21:08:35, 1119.34s/trial, best loss: 32894.62890625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AC996790> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AC996790>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AC996790> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180AC996790>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                    \n",
      "7                                                                                  \n",
      "{'neurons1': 928, 'neurons2': 576, 'acti1v': 'leaky', 'acti2v': 'prelu', 'alphav1': 0.05057968893426812, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.21099660541107612, 'recurrent_dropout2': 0.29327102196615873, 'lr': 0.0001, 'batch_size': 32}\n",
      "=======================================================================================================================================================================================\n",
      " 11%|█         | 8/75 [2:04:34<22:03:07, 1184.89s/trial, best loss: 17212.298828125]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180B9F92700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180B9F92700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180B9F92700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180B9F92700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                     \n",
      "7                                                                                   \n",
      "{'neurons1': 736, 'neurons2': 640, 'acti1v': 'tanh', 'acti2v': 'tanh', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.886357544266182, 'recurrent_dropout2': 0.26007485780590145, 'lr': 0.0001, 'batch_size': 64}\n",
      "=======================================================================================================================================================================================\n",
      " 12%|█▏        | 9/75 [2:15:44<18:46:16, 1023.89s/trial, best loss: 17212.298828125]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BF54E820> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BF54E820>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BF54E820> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BF54E820>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                     \n",
      "9                                                                                   \n",
      "{'neurons1': 256, 'neurons2': 928, 'acti1v': 'tanh', 'acti2v': 'prelu', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.3686428231765312, 'recurrent_dropout2': 0.3840653655364614, 'lr': 0.0001, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      " 13%|█▎        | 10/75 [2:43:38<22:06:46, 1224.71s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C29D8700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C29D8700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C29D8700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C29D8700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                    \n",
      "9                                                                                  \n",
      "{'neurons1': 288, 'neurons2': 992, 'acti1v': 'leaky', 'acti2v': 'prelu', 'alphav1': 0.12735371006181195, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.2597439626554021, 'recurrent_dropout2': 0.5976826041110475, 'lr': 0.0001, 'batch_size': 32}\n",
      "=======================================================================================================================================================================================\n",
      " 15%|█▍        | 11/75 [3:02:15<21:11:19, 1191.86s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C35DC700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C35DC700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C35DC700> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C35DC700>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                    \n",
      "9                                                                                  \n",
      "{'neurons1': 256, 'neurons2': 288, 'acti1v': 'prelu', 'acti2v': 'tanh', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.5788302926379603, 'recurrent_dropout2': 0.2467344568500468, 'lr': 0.0005, 'batch_size': 112}\n",
      "=======================================================================================================================================================================================\n",
      " 16%|█▌        | 12/75 [3:04:06<15:06:12, 863.06s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C6D5C310> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C6D5C310>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C6D5C310> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180C6D5C310>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "9                                                                                 \n",
      "{'neurons1': 736, 'neurons2': 864, 'acti1v': 'prelu', 'acti2v': 'prelu', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.8170853446209649, 'recurrent_dropout2': 0.4591762627834194, 'lr': 0.0001, 'batch_size': 80}\n",
      "=======================================================================================================================================================================================\n",
      " 17%|█▋        | 13/75 [3:18:06<14:44:35, 856.05s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018103E48790> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018103E48790>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018103E48790> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018103E48790>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "9                                                                                 \n",
      "{'neurons1': 544, 'neurons2': 768, 'acti1v': 'tanh', 'acti2v': 'tanh', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.1390454080595812, 'recurrent_dropout2': 0.7604011901070366, 'lr': 0.005, 'batch_size': 64}\n",
      "=======================================================================================================================================================================================\n",
      " 19%|█▊        | 14/75 [3:28:31<13:19:12, 786.11s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A130F70> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A130F70>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A130F70> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A130F70>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "9                                                                                 \n",
      "{'neurons1': 736, 'neurons2': 288, 'acti1v': 'prelu', 'acti2v': 'leaky', 'alphav1': 0, 'alphav2': 0.0016390576776247334, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.7914598708551361, 'recurrent_dropout2': 0.49548837015629166, 'lr': 0.005, 'batch_size': 48}\n",
      "=======================================================================================================================================================================================\n",
      " 20%|██        | 15/75 [3:36:46<11:38:39, 698.65s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018117190430> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018117190430>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018117190430> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018117190430>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "9                                                                                 \n",
      "{'neurons1': 512, 'neurons2': 288, 'acti1v': 'tanh', 'acti2v': 'tanh', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.5390327555873718, 'recurrent_dropout2': 0.14888720687254753, 'lr': 0.001, 'batch_size': 96}\n",
      "=======================================================================================================================================================================================\n",
      " 21%|██▏       | 16/75 [3:41:00<9:15:09, 564.58s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018120D670D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018120D670D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018120D670D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018120D670D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                  \n",
      "9                                                                                \n",
      "{'neurons1': 288, 'neurons2': 800, 'acti1v': 'leaky', 'acti2v': 'tanh', 'alphav1': 0.7257397294245964, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.7594966335745877, 'recurrent_dropout2': 0.4807348952395162, 'lr': 0.001, 'batch_size': 64}\n",
      "=======================================================================================================================================================================================\n",
      " 23%|██▎       | 17/75 [3:49:25<8:48:27, 546.68s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018122B26D30> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018122B26D30>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018122B26D30> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018122B26D30>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                  \n",
      "9                                                                                \n",
      "{'neurons1': 768, 'neurons2': 864, 'acti1v': 'tanh', 'acti2v': 'leaky', 'alphav1': 0, 'alphav2': 0.09789685256589542, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.4171187464020557, 'recurrent_dropout2': 0.5384236484916557, 'lr': 0.0001, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      " 24%|██▍       | 18/75 [4:29:29<17:29:38, 1104.88s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46280> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46280>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46280> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46280>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                    \n",
      "9                                                                                  \n",
      "{'neurons1': 736, 'neurons2': 384, 'acti1v': 'tanh', 'acti2v': 'tanh', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA385D00>, 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.8881990744465614, 'recurrent_dropout2': 0.38757827904833575, 'lr': 0.005, 'batch_size': 96}\n",
      "=======================================================================================================================================================================================\n",
      " 25%|██▌       | 19/75 [4:37:50<14:21:53, 923.46s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46B80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46B80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46B80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x00000180BBC46B80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "9                                                                                 \n",
      "{'neurons1': 640, 'neurons2': 256, 'acti1v': 'tanh', 'acti2v': 'tanh', 'alphav1': 0, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330A60>, 'recurrent_initializer1': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330880>, 'recurrent_initializer2': 'glorot_uniform', 'recurrent_dropout1': 0.9390872577264195, 'recurrent_dropout2': 0.6182517853257553, 'lr': 0.001, 'batch_size': 48}\n",
      "=======================================================================================================================================================================================\n",
      " 27%|██▋       | 20/75 [4:46:23<12:13:33, 800.24s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018129E0C550> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018129E0C550>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018129E0C550> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018129E0C550>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "9                                                                                 \n",
      "{'neurons1': 928, 'neurons2': 576, 'acti1v': 'leaky', 'acti2v': 'prelu', 'alphav1': 0.040553447342841734, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.37389809020816256, 'recurrent_dropout2': 0.36380467174435405, 'lr': 0.0001, 'batch_size': 32}\n",
      "=======================================================================================================================================================================================\n",
      " 28%|██▊       | 21/75 [5:10:17<14:51:35, 990.66s/trial, best loss: 16205.0390625]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001813E2DEAF0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001813E2DEAF0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001813E2DEAF0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001813E2DEAF0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                   \n",
      "21                                                                                \n",
      "{'neurons1': 832, 'neurons2': 704, 'acti1v': 'leaky', 'acti2v': 'prelu', 'alphav1': 0.6470041329772056, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.1839635970272644, 'recurrent_dropout2': 0.06656172522354856, 'lr': 0.0001, 'batch_size': 32}\n",
      "=======================================================================================================================================================================================\n",
      " 29%|██▉       | 22/75 [5:35:58<17:00:52, 1155.72s/trial, best loss: 15047.64453125]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001813B0920D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001813B0920D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001813B0920D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001813B0920D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                     \n",
      "22                                                                                  \n",
      "{'neurons1': 832, 'neurons2': 928, 'acti1v': 'leaky', 'acti2v': 'prelu', 'alphav1': 0.6010056628573185, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.052890236139572555, 'recurrent_dropout2': 0.03331879553141187, 'lr': 0.0001, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      " 31%|███       | 23/75 [6:23:27<24:02:02, 1663.89s/trial, best loss: 8614.1318359375]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A210AF0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A210AF0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A210AF0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x000001810A210AF0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                      \n",
      "22                                                                                   \n",
      "{'neurons1': 832, 'neurons2': 704, 'acti1v': 'leaky', 'acti2v': 'prelu', 'alphav1': 0.6302352262780163, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.008146161884087566, 'recurrent_dropout2': 0.06821490488134531, 'lr': 0.0001, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      " 32%|███▏      | 24/75 [7:02:55<26:33:54, 1875.19s/trial, best loss: 8614.1318359375]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9B9D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9B9D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9B9D0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9B9D0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                      \n",
      "24                                                                                   \n",
      "{'neurons1': 832, 'neurons2': 416, 'acti1v': 'leaky', 'acti2v': 'prelu', 'alphav1': 0.6261500406693793, 'alphav2': 0, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.01855263546915787, 'recurrent_dropout2': 0.015495989646158718, 'lr': 0.0001, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      " 33%|███▎      | 25/75 [7:31:37<25:24:24, 1829.30s/trial, best loss: 7498.787109375]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9BA60> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9BA60>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9BA60> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018146C9BA60>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                     \n",
      "25                                                                                  \n",
      "{'neurons1': 832, 'neurons2': 416, 'acti1v': 'leaky', 'acti2v': 'leaky', 'alphav1': 0.33117455742984053, 'alphav2': 0.36867707858732535, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.07541948735203476, 'recurrent_dropout2': 0.013595679845464392, 'lr': 0.0005, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      " 35%|███▍      | 26/75 [8:01:11<24:40:17, 1812.60s/trial, best loss: 4292.36767578125]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018165A35C10> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018165A35C10>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018165A35C10> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018165A35C10>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "El trial con menor error es el:                                                       \n",
      "25                                                                                    \n",
      "{'neurons1': 800, 'neurons2': 416, 'acti1v': 'leaky', 'acti2v': 'leaky', 'alphav1': 0.3191047009887822, 'alphav2': 0.32139019707925554, 'kernel_initializer1': 'glorot_uniform', 'kernel_initializer2': 'glorot_uniform', 'recurrent_initializer1': 'glorot_uniform', 'recurrent_initializer2': <keras.initializers.initializers_v2.Orthogonal object at 0x00000180FA330190>, 'recurrent_dropout1': 0.09006792718599793, 'recurrent_dropout2': 0.11481445079200008, 'lr': 0.0005, 'batch_size': 16}\n",
      "=======================================================================================================================================================================================\n",
      " 36%|███▌      | 27/75 [8:29:06<23:37:04, 1771.35s/trial, best loss: 4292.36767578125]WARNING:tensorflow:AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018168BC15E0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018168BC15E0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function keras_fmin_fnct.<locals>.customLoss at 0x0000018168BC15E0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function keras_fmin_fnct.<locals>.customLoss at 0x0000018168BC15E0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 36%|███▌      | 27/75 [8:30:58<15:08:24, 1135.52s/trial, best loss: 4292.36767578125]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhyperopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trials, STATUS_OK, tpe\n\u001b[0;32m    119\u001b[0m X_train, y_train, lista_resultados \u001b[38;5;241m=\u001b[39m data()\n\u001b[1;32m--> 120\u001b[0m best_run, best_model\u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnotebook_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbidirectional_lstm_hyperas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_model, best_run)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperas\\optim.py:59\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(model,\n\u001b[0;32m     21\u001b[0m              data,\n\u001b[0;32m     22\u001b[0m              algo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m              return_space\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m              keep_temp\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m    Minimize a keras model for given data and implicit hyperparameters.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m    If `return_space` is True: The pair of best result and corresponding keras model, and the hyperopt search space\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     best_run, space \u001b[39m=\u001b[39m base_minimizer(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     60\u001b[0m                                      data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m     61\u001b[0m                                      functions\u001b[39m=\u001b[39;49mfunctions,\n\u001b[0;32m     62\u001b[0m                                      algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m     63\u001b[0m                                      max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m     64\u001b[0m                                      trials\u001b[39m=\u001b[39;49mtrials,\n\u001b[0;32m     65\u001b[0m                                      rseed\u001b[39m=\u001b[39;49mrseed,\n\u001b[0;32m     66\u001b[0m                                      full_model_string\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     67\u001b[0m                                      notebook_name\u001b[39m=\u001b[39;49mnotebook_name,\n\u001b[0;32m     68\u001b[0m                                      verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m     69\u001b[0m                                      keep_temp\u001b[39m=\u001b[39;49mkeep_temp)\n\u001b[0;32m     71\u001b[0m     best_model \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m trials:\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperas\\optim.py:133\u001b[0m, in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 133\u001b[0m     fmin(keras_fmin_fnct,\n\u001b[0;32m    134\u001b[0m          space\u001b[39m=\u001b[39;49mget_space(),\n\u001b[0;32m    135\u001b[0m          algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    136\u001b[0m          max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    137\u001b[0m          trials\u001b[39m=\u001b[39;49mtrials,\n\u001b[0;32m    138\u001b[0m          rstate\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mRandomState(rseed),\n\u001b[0;32m    139\u001b[0m          return_argmin\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m    140\u001b[0m     get_space()\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperopt\\fmin.py:507\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    504\u001b[0m validate_loss_threshold(loss_threshold)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 507\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[0;32m    508\u001b[0m         fn,\n\u001b[0;32m    509\u001b[0m         space,\n\u001b[0;32m    510\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[0;32m    511\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    512\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    513\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    514\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    515\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    516\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    517\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    518\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    519\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    520\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    521\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    522\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperopt\\base.py:682\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[1;32m--> 682\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[0;32m    683\u001b[0m     fn,\n\u001b[0;32m    684\u001b[0m     space,\n\u001b[0;32m    685\u001b[0m     algo,\n\u001b[0;32m    686\u001b[0m     max_evals,\n\u001b[0;32m    687\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    688\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[0;32m    689\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    690\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[0;32m    691\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    692\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[0;32m    693\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    694\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[0;32m    695\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[0;32m    696\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[0;32m    697\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[0;32m    698\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[0;32m    699\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[0;32m    700\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperopt\\fmin.py:553\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    550\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    552\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[0;32m    555\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[0;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperopt\\fmin.py:356\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[1;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[0;32m    357\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperopt\\fmin.py:292\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    289\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[0;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperopt\\fmin.py:170\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    168\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m    169\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[0;32m    171\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    172\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\hyperopt\\base.py:907\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    899\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    900\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    902\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[0;32m    903\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[0;32m    904\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[0;32m    905\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    906\u001b[0m     )\n\u001b[1;32m--> 907\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[0;32m    910\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32mg:\\Mi unidad\\00_AÑOS ACADEMICOS\\4º AÑO BDATA\\RETOS\\reto12\\code\\temp_model.py:181\u001b[0m, in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\anaconda3\\envs\\reto12\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "def data():\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from keras import backend as K\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    path_origin, path_processed, models_path, cells = \"../data/original_data/\", \"../data/processed_data/\", \"../data/processed_data/models/\", [f'\\Cell{x}\\*' for x in range(1, 9)]\n",
    "\n",
    "    df_desc_final_pickle_load = pd.read_pickle(path_processed + 'df_desc_final.pkl')\n",
    "    df_desc_final_pickle = df_desc_final_pickle_load[df_desc_final_pickle_load.RUL >= 0 ]\n",
    "    df_desc_final_pickle[\"RUL\"] = df_desc_final_pickle[\"RUL\"].astype(float)\n",
    "\n",
    "    x, y, num_steps, percentage_and_rul = [], [], 10, []\n",
    "\n",
    "    for i in range(1, 9):\n",
    "        batery_df = df_desc_final_pickle[df_desc_final_pickle.index.get_level_values(0) == i]\n",
    "        batery_df.index = pd.MultiIndex.from_tuples([(1, x) for x in range(1, batery_df.shape[0] + 1)])\n",
    "        for j in range(1, len(batery_df) - num_steps):\n",
    "            x.append(np.array(batery_df.loc[(1, slice(j,num_steps+j)), :].drop(columns=['RUL']).values))\n",
    "            y.append(np.array([batery_df.loc[(1, slice(j,num_steps+j)), :].RUL.values[-1], i]))\n",
    "            \n",
    "    x, y = np.array(x), np.array(y)\n",
    "    X_train,X_test,y_train_mod,y_test_mod = train_test_split(x, y,test_size=0.2, random_state=42, stratify=y[:,1])\n",
    "    y_train, y_test = y_train_mod[:,0].reshape(-1,1), y_test_mod[:,0].reshape(-1,1)\n",
    "    X_train, y_train = np.stack(X_train, axis = 0), np.stack(y_train, axis = 0)\n",
    "    X_test, y_test = np.stack(X_test, axis = 0), np.stack(y_test, axis = 0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "    lista_resultados = []\n",
    "    return X_train,  y_train,  lista_resultados \n",
    "\n",
    "def create_model(X_train, y_train, lista_resultados):\n",
    "\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    def customLoss(true,pred):\n",
    "        diff = pred - true\n",
    "        greater = K.greater(diff,0)\n",
    "        greater = K.cast(greater, K.floatx()) \n",
    "        greater = greater + 1                \n",
    "        return K.mean(K.square(diff))*greater\n",
    "    \n",
    "    nombre = models_path + 'tryals_bidirectional_lstm_' + str(len(lista_resultados)) + '.h5'\n",
    "    checkpoint_rb = keras.callbacks.ModelCheckpoint(nombre, verbose=0, save_best_only=True)\n",
    "\n",
    "    alphav1, alphav2 = 0, 0\n",
    "    acti1v = {{ choice(['leaky' , 'prelu', 'tanh'])}}\n",
    "    if acti1v == 'leaky':  \n",
    "        alphav1 = {{uniform(0, 1)}}\n",
    "        acti1 = keras.layers.LeakyReLU(alpha=alphav1)\n",
    "    if acti1v == 'prelu': acti1 = tf.keras.layers.PReLU('zeros')\n",
    "    if acti1v == 'relu': acti1 = 'relu'\n",
    "    if acti1v == 'tanh': acti1 = 'tanh'\n",
    "\n",
    "    acti2v = {{ choice(['leaky','prelu','tanh'])}}\n",
    "    if acti2v == 'leaky':  \n",
    "        alphav2 = {{uniform(0, 1)}}\n",
    "        acti2 = keras.layers.LeakyReLU(alpha=alphav2)\n",
    "    if acti2v == 'prelu': acti2 = tf.keras.layers.PReLU('zeros')\n",
    "    if acti2v == 'relu': acti2 = 'relu'\n",
    "    if acti2v == 'tanh': acti2 = 'tanh'\n",
    "\n",
    "    kernel_initializer1, kernel_initializer2 = {{choice( [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] )}}, {{choice( [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] )}}\n",
    "    recurrent_initializer1, recurrent_initializer2 = {{choice( [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] )}}, {{choice( [\"glorot_uniform\", tf.keras.initializers.Orthogonal()] )}}\n",
    "    recurrent_dropout1, recurrent_dropout2 = {{uniform(0, 1)}}, {{uniform(0, 1)}}\n",
    "    neurons1, neurons2 = {{choice([x for x in range(256, 1024, 32)])}}, {{choice([x for x in range(256, 1024, 32)])}}\n",
    "    np.random.seed(26), tf.random.set_seed(26);\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    forward_layer = keras.layers.LSTM(neurons1, activation = acti1, kernel_initializer=kernel_initializer1, recurrent_initializer=recurrent_initializer1, \n",
    "                                        recurrent_dropout = recurrent_dropout1)\n",
    "    backward_layer = keras.layers.LSTM(neurons2, activation = acti2, kernel_initializer=kernel_initializer2, recurrent_initializer=recurrent_initializer2, \n",
    "                                        recurrent_dropout = recurrent_dropout2, go_backwards = True)\n",
    "\n",
    "    model.add(keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer, input_shape=(X_train.shape[1], X_train.shape[2])  ))\n",
    "    model.add(keras.layers.Dense(1, activation = 'relu'))\n",
    "\n",
    "    lr = {{choice([0.005,0.001,0.0005, 0.0001])}}\n",
    "    model.compile(loss = customLoss,  optimizer = keras.optimizers.Adam(learning_rate = lr), metrics = ['mse'])\n",
    "    \n",
    "    print('=======================================================================================================================================================================================')\n",
    "    batch_size = {{choice([x for x in range(16, 128, 16)])}}\n",
    "    history = model.fit(X_train, y_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs=500, validation_split=0.2, verbose=0, callbacks = [checkpoint_rb])\n",
    "\n",
    "    val_acc_per_epoch = history.history['val_loss']\n",
    "    best_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch))\n",
    "    lista_resultados.append(history.history['val_loss'][best_epoch])\n",
    "    print('El trial con menor error es el:', np.argmin(lista_resultados))\n",
    "    params_dic = {'neurons1': neurons1, 'neurons2': neurons2, 'acti1v': acti1v, 'acti2v': acti2v, 'alphav1': alphav1, 'alphav2': alphav2, 'kernel_initializer1': kernel_initializer1,\n",
    "                    'kernel_initializer2': kernel_initializer2, 'recurrent_initializer1': recurrent_initializer1, 'recurrent_initializer2': recurrent_initializer2,\n",
    "                    'recurrent_dropout1': recurrent_dropout1, 'recurrent_dropout2': recurrent_dropout2, 'lr': lr, 'batch_size': batch_size}\n",
    "    print(params_dic)\n",
    "    \n",
    "    return {'loss' : history.history['val_loss'][best_epoch], 'status' : STATUS_OK, 'model' : model, 'metrics' : np.sqrt(history.history['val_mse'][best_epoch])}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from hyperas import optim\n",
    "    from hyperas.distributions import choice, uniform\n",
    "    from hyperopt import Trials, STATUS_OK, tpe\n",
    "    X_train, y_train, lista_resultados = data()\n",
    "    best_run, best_model= optim.minimize(model=create_model,\n",
    "                                         data=data,\n",
    "                                         algo=tpe.suggest,\n",
    "                                         max_evals=75,\n",
    "                                         trials=Trials(),\n",
    "                                         notebook_name='bidirectional_lstm_hyperas')\n",
    "    print(best_model, best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 360)               524520    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 360)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 361       \n",
      "=================================================================\n",
      "Total params: 524,881\n",
      "Trainable params: 524,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dropout': 0.02733288830118466,\n",
       " 'LSTM': 5,\n",
       " 'activ': 1,\n",
       " 'alpha': 0.3799718452885261,\n",
       " 'batch_size': 0,\n",
       " 'constrain': 1,\n",
       " 'kernel_initializer': 0,\n",
       " 'max_value': 4.132771265163272}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('reto12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f16b7001b204c41f920b02e28f3ff8f9b0ca8507f82933210dd898ae15876d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
